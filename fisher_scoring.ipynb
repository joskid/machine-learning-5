{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's Scoring for Logistic Regression\n",
    "\n",
    "In this notebook, we implement Fisher's Scoring by using the Newton-Raphson optimization method to mimize the empirical loss for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the average empirical loss (the risk) for logistic regression on $m$ training examples, given by\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\log\\left(1 + e^{-y^{(i)}\\theta^Tx^{(i)}}\\right) = -\\frac{1}{m} \\sum_{i=1}^m \\log(h_\\theta(y^{(i)}x^{(i)}))\n",
    "$$\n",
    "\n",
    "where the output $y^{(i)} \\in \\{-1, 1\\}$, and the hypothesis is of the form $h_\\theta(x) = g(\\theta^T x)$ with $g(z) = 1/(1+e^{-z})$ being the sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by finding the gradient of $J(\\theta)$ with respect to the parameter vector $\\theta$. For all $j=\\{1,2,...,n\\}$, \n",
    "\n",
    "$$\\begin{align*}\n",
    "(\\nabla_\\theta J(\\theta))_j = \\frac{\\partial}{\\partial\\theta_j} J(\\theta)\n",
    "&= \\frac{\\partial}{\\partial\\theta_j} \\left[-\\frac{1}{m} \\sum_{i=1}^m \\log(h_\\theta(y^{(i)}x^{(i)}))\\right] \\\\\n",
    "&= \\frac{\\partial}{\\partial\\theta_j} \\left[-\\frac{1}{m} \\sum_{i=1}^m \\log g(y^{(i)}\\theta^T x^{(i)})\\right] \\\\\n",
    "&= -\\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial}{\\partial\\theta_j} \\log g(y^{(i)}\\theta^T x^{(i)}) \\\\\n",
    "&= -\\frac{1}{m} \\sum_{i=1}^m \\left[g(y^{(i)}\\theta^T x^{(i)}) \\right]^{-1} g'(y^{(i)}\\theta^T x^{(i)}) \\frac{\\partial}{\\partial\\theta_j} y^{(i)} \\sum_{k=1}^n \\theta_k x_k^{(i)} \\\\\n",
    "&= -\\frac{1}{m} \\sum_{i=1}^m \\left[g(y^{(i)}\\theta^T x^{(i)}) \\right]^{-1} g(y^{(i)}\\theta^T x^{(i)})\\left[1 - g(y^{(i)}\\theta^T x^{(i)})\\right] y^{(i)}x_j^{(i)} \\\\\n",
    "&= -\\frac{1}{m} \\sum_{i=1}^m \\left[1 - g(y^{(i)}\\theta^T x^{(i)})\\right] y^{(i)}x_j^{(i)},\n",
    "\\end{align*}$$\n",
    "\n",
    "where the fourth equality uses the distributive property of partial differentiation, and the sixth equality uses the convenient fact that for the sigmoid function, $g'(z) = g(z)[1-g(z)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the Hessian of $J(\\theta)$ with respect to $\\theta$. For all $j, k = \\{1,2,...,n\\}$,\n",
    "\n",
    "$$\\begin{align*}\n",
    "(\\nabla_\\theta^2 J(\\theta))_{jk} = \\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_k} J(\\theta) &= \n",
    "\\frac{\\partial}{\\partial\\theta_k}\\left[\\frac{\\partial}{\\partial\\theta_j} J(\\theta) \\right] \\\\\n",
    "&= \\frac{\\partial}{\\partial\\theta_k} \\left(-\\frac{1}{m}\\sum_{i=1}^m \\left[1-g(y^{(i)}\\theta^T x^{(i)}\\right] y^{(i)}x_j^{(i)}\\right) \\\\\n",
    "&= -\\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial}{\\partial\\theta_k} \\left[y^{(i)}x_j^{(i)} - y^{(i)}x_j^{(i)}g\\left(y^{(i)}\\theta^T x^{(i)}\\right)\\right] \\\\\n",
    "&= -\\frac{1}{m} \\sum_{i=1}^m \\left[0 - y^{(i)}x_j^{(i)} g'\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\frac{\\partial}{\\partial\\theta_k} y^{(i)} \\sum_{l=1}^n\\theta_l x_l^{(i)} \\right] \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^m g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\left[1 - g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\right] (y^{(i)})^2 x_j^{(i)} x_k^{(i)} \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^m g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\left[1 - g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\right] x_j^{(i)} x_k^{(i)}, \n",
    "\\end{align*}$$\n",
    "\n",
    "where the fifth equality uses the same property $g'(z) = g(z)\\left[1-g(z)\\right]$ of the sigmoid function. Notice that the expression $x_j^{(i)} x_k^{(i)}$ for all $j, k = \\{1,2,...,n\\}$ corresponds to the outer product between the vectors $x^{(i)}\\left(x^{(i)}\\right)^T$. It follows that the above can be written as\n",
    "\n",
    "$$\n",
    "H = \\nabla_\\theta^2 J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\left[1 - g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\right] x^{(i)} \\left(x^{(i)}\\right)^T.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any minimization algorithm to work on the loss function $J(\\theta)$, $J(\\theta)$ needs to be convex. It turns out that $J(\\theta)$ is convex if its Hessian $H$ is positive semi-definite, which we can easily show. Notice that by definition of the sigmoid function, $0 \\leq g(z) \\leq 1$ for all $z\\in \\mathbb{R}$, and so consequently $0 \\leq g(z)\\left[1-g(z)\\right] \\leq 1$. Now, let $z \\in \\mathbb{R}$ be given. Then,\n",
    "\n",
    "$$\\begin{align*}\n",
    "z^T H z &= z^T \\left(\\frac{1}{m}\\sum_{i=1}^m g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\left[1 - g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\right] x^{(i)} \\left(x^{(i)}\\right)^T\\right) z \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^m g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\left[1 - g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\right] z^T x^{(i)} \\left(x^{(i)}\\right)^T z \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^m g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\left[1 - g\\left(y^{(i)}\\theta^T x^{(i)}\\right) \\right] \\left((x^{(i)})^T z\\right)^2 \\geq 0,\n",
    "\\end{align*}$$\n",
    "\n",
    "where the last inequality follows because $0 \\leq g(z)\\left[1-g(z)\\right] \\leq 1$ for all $z\\in \\mathbb{R}$ as noted earlier and $\\left((x^{(i)})^T z\\right)^2$ is always a non-negative quantity. Because $z\\in \\mathbb{R}$ was arbitrary, it follows that $H$ is positive semi-definite, and so $J(\\theta)$ is convex i.e. $J(\\theta)$ has only one (global) minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are in a position to implement logistic regression on a sample dataset. First, we'll import the following libraries that will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np               # for numerical operations\n",
    "import scipy.special as sp       # for the sigmoid function\n",
    "import matplotlib.pyplot as plt  # to visualize the logistic regression decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll read in the input and output files (downloadable from http://cs229.stanford.edu/ps/ps1) as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_raw = np.genfromtxt('logistic_x.txt', dtype = float)\n",
    "Y = np.genfromtxt('logistic_y.txt', dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll concatenate a column of 1s to the input array to represent the bias, and visualize the first five rows of the input array i.e. the features of the first five training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.3432504  -1.3311479 ]\n",
      " [ 1.          1.8205529  -0.6346681 ]\n",
      " [ 1.          0.98632067 -1.8885762 ]\n",
      " [ 1.          1.9443734  -1.635452  ]\n",
      " [ 1.          0.97673352 -1.3533151 ]]\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((np.ones((99, 1)), X_raw), axis = 1)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we'll store the number of training examples, and initialize all parameters to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = X.shape[0]\n",
    "theta = np.zeros(3)  # 2 features plus 1 bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll implement the main training loop that performs the parameter update, given by\n",
    "\n",
    "$$\\theta := \\theta - H^{-1}\\nabla_\\theta J(\\theta).$$\n",
    "\n",
    "To avoid nested for loops within each training iteration, we will vectorize the operations by computing the components of the above update rule for all training examples simulatneously. This can be done efficiently using numpy methods, as shown below. Recall that we have derived the gradient and Hessian of the loss function $J(\\theta)$ with respect to the parameter vector $\\theta$ in the workings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.37472471\n",
      "Loss is 0.33425014\n",
      "Loss is 0.32928120\n",
      "Loss is 0.32914756\n",
      "Loss is 0.32914743\n",
      "Loss is 0.32914743\n",
      "Loss is 0.32914743\n",
      "Loss is 0.32914743\n",
      "Loss is 0.32914743\n",
      "Loss is 0.32914743\n"
     ]
    }
   ],
   "source": [
    "# about 10 training iterations suffice\n",
    "for _ in range(10):\n",
    "    # compute the vectorized output of the sigmoid function\n",
    "    g = sp.expit(Y * np.sum(theta * X, axis = 1))\n",
    "    # compute the gradient using all training examples\n",
    "    grad = -(1/M) * np.sum((np.reshape((np.ones(M) - g) * Y, newshape = (99, 1))) * X, axis = 0)\n",
    "    # use a for loop to construct the hessian using all training examples\n",
    "    mat = np.empty((99, 0))\n",
    "    for j in range(len(theta)):\n",
    "        for k in range(len(theta)):\n",
    "            mat = np.append(mat, X[:,[j]] * X[:,[k]], axis = 1)\n",
    "    hess_flat = (1/M) * np.sum(np.reshape(g * (np.ones(M) - g), newshape = (99, 1)) * mat, axis = 0)\n",
    "    hess = np.reshape(hess_flat, newshape = (len(theta), len(theta)))\n",
    "    # perform parameter update\n",
    "    theta = theta - np.dot(np.linalg.inv(hess), grad)\n",
    "    # compute and print loss\n",
    "    loss = (1/M) * np.sum(np.log(1 + np.exp(-Y * np.sum(theta * X, axis = 1))), axis = 0)\n",
    "    print(\"Loss is %.8f\" % loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the average empirical loss decreases with each iteration, and seems to converge quickly in under 10 iterations. The trained parameters after 10 iterations are printed below. These correspond to the weights associated with the bias, $x_1$ and $x_2$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained parameters are (bias first):  [-2.6205116   0.76037154  1.17194674]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained parameters are (bias first): \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish by visualizing the learnt decision boundary along with the training examples and their true classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwRhjjO//A2+A6ca15j7gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23c1c545358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot classification of training data\n",
    "plt.plot(X[Y==1, 1], X[Y==1 ,2], \"r_\", label = \"-1\")\n",
    "plt.plot(X[Y==-1, 1], X[Y==-1, 2], \"b+\", label = \"+1\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\t\n",
    "plt.legend()\n",
    "\n",
    "# define function to plot decision boundary\n",
    "def graph(formula, x_range):\n",
    "    x = np.array(x_range)\n",
    "    y = eval(formula)\n",
    "    plt.plot(x, y, \"g--\")\n",
    "\n",
    "# plot decision boundary and show complete plot\n",
    "graph(formula = \"(1/theta[2]) * (-theta[0] - theta[1] * x)\", x_range = range(0, 9))\n",
    "plt.title(\"Decision boundary for logistic regression\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
