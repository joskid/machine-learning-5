---
title: "Final Exam"
author: "St. Clair"
date: "Math 315 (Data Science) - due by 5pm Monday, 11/21/16"
output:
  rmarkdown::github_document:
    fig_height: 4
    fig_width: 8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, prompt=TRUE,comment=NULL,message=FALSE, 
                      include=TRUE, cache=TRUE)
```

```{r packageCheck, include=FALSE}
mypacks <- c("dplyr","ggplot2","tidyr","readr","stringr","lubridate","statnet","circlize")  # what packages are needed?
packs <- installed.packages()   # find installed package list
install.me <- mypacks[!(mypacks %in% packs[,"Package"])]  #what needs to be installed?
if (length(install.me) >= 1) install.packages(install.me, repos = "http://cran.us.r-project.org")   # install (if needed)
lapply(mypacks, library, character.only=TRUE)  # load all packages
```

## Your Name: Il Shan Ng

Honesty: You can only discuss this exam with Prof. St. Clair! You can use any other non-person resource available to you, including your textbook(s), R scripts, and the internet. 

This exam is worth 130 points. There are 10 "non-challenge" problems below worth 10 points each. Most of these are straightforward problems similar to problems done in class. The two "challenge" problems are worth 15 points each. As the name implies, these are intended to be challenging and stretch your data manipulation and problem solving skills. If you get stuck, just show the progress that you made on (any) problem for (hopefully) some partial credit. For each problem, your errors do not accumulate so a mistake, say, part (a) should not cost you additional points in parts that follow. Submit your **.Rmd and HTML** files to your Hand-in Courses folder by 5pm Monday, 11/21/16.


### Problem 1 
The datasets below contain information on book ratings pulled from book review web site during in a four week period in 2004. The files contain user info (data on the rater), book info and rating info (0= bad to 10=best).
```{r, eval=TRUE}
users <-  read_csv("http://people.carleton.edu/~kstclair/data/bookUsers.csv", na=c("","NA","NULL")) # user data
books <-  read_csv("http://people.carleton.edu/~kstclair/data/Books.csv")  # book names and other info
ratings <- read_csv("http://people.carleton.edu/~kstclair/data/bookRatings.csv") # book reviews (ratings)
```
Use these data sets to answer the questions below. 

#### (1a)
Provide the following info about this data:

- number of (unique) books reviewed (given a rating) as identified by their written book title

```{r}
books_ratings_right <- books %>%
  right_join(ratings, by = "ISBN")

(num_books <- books_ratings_right %>% 
  filter(!is.na(Book.Title)) %>%
  select(Book.Title) %>% 
  n_distinct())
```

*answer:* There are `r num_books` unique books reviewed, as identified by the book title.

- number of (unique) books reviewed (given a rating) as identified by their ISBN

```{r}
(num_ISBN <- ratings %>% select(ISBN) %>% n_distinct())
```

*answer:* There are `r num_ISBN` unique books reviewed, as identified by the ISBN.

- number of (unique) users who provided a rating

```{r}
(num_users <- ratings %>% select(User.ID) %>% n_distinct())
```

*answer:* There are `r num_users` unique users who provided a rating.

#### (1b)
Consider how to identify books, either via ISBN or by their title. For books with a rating:

- How many books with no title have an ISBN?

```{r}
(num_noTitle <- books_ratings_right %>% 
  filter(is.na(Book.Title), !is.na(ISBN)) %>%
  select(ISBN) %>%
  n_distinct())
```

*answer:* There are `r num_noTitle` books that have no title but have ISBNs.

- What is the largest number of ISBN's associated with a book title? What is the name of the book? 
```{r}
IBSN_count <- books_ratings_right %>%
  group_by(Book.Title, Book.Author) %>%
  summarise(n = n(), ISBN_count = n_distinct(ISBN)) %>%
  arrange(desc(ISBN_count))
```

*answer:* The book titled "`r IBSN_count[2,1]`" has the largest number, `r IBSN_count[2,4]`, of associated IBSNs. 

- Are there any ISBN's with more than one book title associated with it? 

```{r}
books_ratings_right %>%
  group_by(ISBN) %>%
  summarise(n = n(), title_count = n_distinct(Book.Title)) %>%
  .$title_count %>%
  max()
```

*answer:* None of the ISBNs have more than one book title associated with it.

#### (1c)
For books with a title, 

- Find the name of the book with the highest average rating for all books with over 50 reviews

```{r}
books_ratings_right2 <- books_ratings_right %>%
  filter(!is.na(Book.Title))          # remove books with no title

avg_ratings <- books_ratings_right2 %>%
  group_by(Book.Title, Book.Author) %>%
  summarise(num_reviews = n(), avg_rating = mean(Book.Rating)) %>%
  filter(num_reviews > 50) %>%
  arrange(desc(avg_rating))
```

*answer:* The book "`r avg_ratings[1,1]`" had the highest average rating for all books with over 50 reviews, with an average score of `r round(avg_ratings[1,4], digits = 4)`.

- Plot the number of reviews that a book has against its average rating, color coded by the number of books that the book's author has in the rating data. Describe the pattern that you see.

```{r, fig.width = 8, fig.height = 4}
authors <- books_ratings_right2 %>%
  group_by(Book.Author) %>%
  summarise(num_ratings = n(), num_books = n_distinct(Book.Title)) %>%
  arrange(desc(num_books))

plot_df <- books_ratings_right2 %>%
  left_join(authors, by = "Book.Author") %>%
  group_by(Book.Title, Book.Author) %>%
  summarise(num_reviews = n(), avg_rating = mean(Book.Rating), author_books = mean(num_books)) %>%
  filter(num_reviews > 50) %>%
  arrange(desc(avg_rating))

ggplot(plot_df, aes(x = num_reviews, y = avg_rating)) +
  geom_point(aes(color = author_books)) +
  stat_smooth(method = "lm") +
  coord_trans(x = "log2") +
  scale_color_gradient(name = "Number of books\nby author", low = "white", high = "red") +
  theme(panel.background = element_rect(fill = "#B3B3B3")) +
  labs(x = "Number of reviews", y = "Average rating")

plot_df %>% filter(num_reviews > 1000) 
plot_df %>% filter(avg_rating > 7)
```

*answer:* A book's average rating is positively correlated with the number of reviews it has gotten. Though less obvious, the number of books written by the same author tends to increase as both the average rating and number of reviews increases. There are two extreme outliers along the horizontal axis -- "The Lovely Bones: A Novel" with 1295 reviews and "Wild Animus" with 2502 reviews. Along the vertical axis, the outlier is "Free" with an average rating of 7.96.

#### (1d)
Use the user `Location` variable to identify the country where the review lives. Then redraw your plot from (1c) but facet by the top 4 countries in terms of the total number of books reviewed by users in that country. What book stands out as an outlier for all four countries?

```{r, fig.width = 10, fig.height = 10}
books_ratings_right3 <- books_ratings_right2 %>%
  left_join(users, by = "User.ID") %>%
  separate(Location, sep = ",\\s", into = c("col1", "col2", "country")) %>%
  select(-c(col1, col2))

(top4 <- books_ratings_right3 %>% 
  count(country) %>% 
  arrange(desc(n)) %>% 
  slice(1:4) %>% 
  .$country)

plot_df2 <- books_ratings_right3 %>% 
  mutate(country = ifelse(country %in% top4, country, "other")) %>%
  left_join(authors, by = "Book.Author") %>%
  group_by(Book.Title, Book.Author, country) %>%
  summarise(num_reviews = n(), avg_rating = mean(Book.Rating), author_books = mean(num_books)) %>%
  filter(country != "other") %>%
  arrange(desc(avg_rating))

ggplot(plot_df2, aes(x = num_reviews, y = avg_rating)) +
  geom_point(aes(color = author_books)) +
  stat_smooth(method = "lm") +
  coord_trans(x = "log2") +
  facet_wrap( ~ country) +
  scale_color_gradient(name = "Number of books\nby author", low = "white", high = "red") +
  theme(panel.background = element_rect(fill = "#B3B3B3")) +
  labs(x = "Number of reviews", y = "Average rating")

plot_df2 %>% group_by(country) %>% arrange(desc(num_reviews)) %>% slice(1)
```

*answer:* The book titled "Wild Animus" has an unusually large number of reviews from all four countries.

### Problem 2 Unsupervised Learning
Load the dataset for problem 2. This dataset has two variables `x` and `y`.

```{r}
d <- read_csv("http://people.carleton.edu/~kstclair/data/p2data.csv")
```

#### (2a)
Make a scatterplot of the two variables in this dataset. Explain why there are clearly two clusters, or groups, of points that follow the same (non-linear) pattern. 

```{r}
ggplot(d, aes(x = x, y = y)) +
  geom_point()
```

*answer:* The first cluster of points starts at about (0.4, 0), then spirals anti-clockwise with increasing radius for 1.5 full circles, ending at about (-1.4, 0.1). The second cluster of points starts at about (-0.4, 0), then spirals anti-clockwise with increasing radius for 1.5 full circles, ending at about (1.4, -0.1). Since these two spirals don't ever intersect, they form two clear clusters. Since they both spiral anti-clockwise with center at the origin and have start and end points that are reflections of each other in the line x = 0, the two clusters clearly follow the same non-linear pattern.

#### (2b)
Fit a k-means clustering algorithm to the data with $k=2$ clusters. Redraw the plot of the data with clusters identified by color. Can this method of identifying clusters work with this type of data? Why or why not?

```{r}
km1 <- kmeans(d, centers = 2, nstart = 20)
d$cluster <- as.character(km1$cluster)
ggplot(d, aes(x = x, y = y)) +
  geom_point(aes(color = cluster))
```

*answer:* The k-means clustering algorithm cannot work with this type of data. The plot shows that the algorithm has divided the points into two clusters using approximately the line y = x. This occurs because the algorithm tries to divide the points into two clusters such that the within cluster variation (a summary measure of the Euclidean distance between points within a cluster) when summed across the two clusters is minimized. Since this relies on points being proximate to each other, it fails to take into account the outward spiralling pattern, which is what defines the two clusters. 

#### (2c)
Fit a hierarchical clustering algorithm to the data then cut the tree into 2 clusters. Draw a scatterplot of the data that is faceted by clustering method (used in b and c) with color denoting the cluster assignment for each method.  Does hierarchical clustering  do any better or worse (or the same) as k-means clustering? Explain. 

```{r}
dist <- dist(scale(d[c("x","y")]))
hc1 <- hclust(dist)
d$hierarchical <- as.character(cutree(hc1, k = 2))

d_narrow <- d %>%
  rename(kmeans = cluster) %>%
  gather(kmeans, hierarchical, key = "model", value = "cluster")

ggplot(d_narrow, aes(x = x, y = y)) +
  geom_point(aes(color = cluster)) +
  facet_wrap( ~ model)
```

*answer:* The hierarchical clustering algorithm does not do any better than the k-means algorithm, since it also partitions the data into two clusters roughly symmetric around the line y = x. This means that it also fails to take into account the spiralling pattern exhibited by each true cluster.

#### (2d) (Challenge)
Explore a better way to try to identify the clusters in this data. Consider either transforming the data and/or writing your own clustering algorithm to find clusters of this nature. You don't need to find an approach that succeeds, just make sure to try something reasonable and clearly explain your approach. 

To earn 10 points on this problem you need to take successfully implement a possible solution. To earn the full 15 points I would like to see 

(1) your solution do better than the methods in (b) and (c), 
(2) you use clear and concise code (i.e. readable) to implement your solution and 
(3) provide a clear explanation of your work

Note: you can explore different clustering algorithms available from different R packages. But if you take this approach, you *must* be able to describe how the function finds clusters and how this differs from the two methods we used in class. I don't want to see you using other clustering functions that you can't explain!

*answer:* I transform the data to account for the spiral shaped pattern of each cluster. First, I decide whether each point lives in the first, second, third or fourth quadrant with respect to the origin. I then calculate the distance between the point and the origin (`radius`) and the base angle (`alpha`) for each point. Using the quadrant information, I calculate the angle that each point makes with respect to the horizontal axis. The steps are shown in the code below, along with a plot of `radius` against `angle`. I have transformed the x,y data into polar coordinate form.

```{r}
d <- read_csv("http://people.carleton.edu/~kstclair/data/p2data.csv")

d2 <- d %>%
  mutate(quad = ifelse(x >= 0 & y >= 0, 1, 
                       ifelse(x < 0 & y >= 0, 2,
                              ifelse(x < 0 & y < 0, 3, 4))),
         radius = sqrt(x^2 + y^2),
         alpha = atan(abs(y)/abs(x)),
         angle = ifelse(quad == 1, alpha,
                        ifelse(quad == 2, pi - alpha,
                               ifelse(quad == 3, pi + alpha, 2*pi - alpha)))
  )

ggplot(d2, aes(x = angle, y = radius)) +
  geom_point() +
  stat_function(fun = function(x) 0.5 + 0.11*x, color = "red") +
  ylim(c(0,1.5))
```

The plot shows four distinct clusters. The first spiral that starts unwinding at (-0.4, 0) corresponds to the second and fourth diagonal bands from the top, while the second spiral that starts unwinding at (0.4, 0) corresponds to the first and third diagonal bands from the top. To use the hierarchical clustering method to identify these four clusters, I will have to "compress" the diagonal bands by means of a linear transformation. More specifically, I fit the red line via trial-and-error (there may be a way to automate this using regression) and find the orthogonal projection of each point onto a line perpendicular to this red line. Finding that the line perpendicular to the red line has a slope of -1/0.11, I use the orthogonal projection formula to compute the transformed versions of angle and radius, as shown below. Shown also is the data after this linear transformation.

```{r}
d2 <- d2 %>%
  mutate(angle2 = (angle-(1/0.11)*radius)/(1+1/0.11^2),
         radius2 = angle2 * -(1/0.11))

ggplot(d2, aes(x = angle2, y = radius2)) +
  geom_point()
```

I then proceed to identify the four clusters with respect to `angle2` and `radius2` using the hierarchical clustering algorithm. I first plot the original data separated into four clusters to check if the data transformation has managed to account for the spiral shaped pattern. It has.

```{r}
dist2 <- dist(scale(d2[c("angle2", "radius2")]))
hc2 <- hclust(dist2)
d2$cluster <- as.character(cutree(hc2, k = 4))

ggplot(d2, aes(x = x, y = y)) +
  geom_point(aes(color = cluster))
```

Finally, I fuse the first and second clusters together, and the third and fourth clusters together. A plot of the data with the fused clusters shows that the transformation has worked well to classify every point correctly but one. 

```{r}
d2 <- d2 %>%
  mutate(cluster2 = ifelse(cluster == "2", "1",
                          ifelse(cluster == "3", "2", 
                                 ifelse(cluster == "4", "2", "1"))))

ggplot(d2, aes(x = x, y = y)) +
  geom_point(aes(color = cluster2))
```

The main limitation of the above process is that the number of clusters created with respect to the orthogonally projected values of angle and radius may change depending on how many full circles the spirals in the data makes. This will then affect the way the initial clusters must be fused into just two clusters that identify the two spirals.

### Problem 3: Networks
Reconsider the migration network example from days 25 and 27. But now we will construct the network by female migration in 1960 (e.g. country A has an edge to country B if the female migration count in 1960 from A to B is non-zero). 

#### (3a) 
Construct a network object for the female 1960 network described above. How many vertices and edges are in this network?

```{r}
MigrationFlows <- read_csv("https://people.carleton.edu/~kstclair/data/MigrationFlows.csv")

MigrationFlowsF <- MigrationFlows %>%
  filter(sex == "Female", Y1960 > 0)

miNet1960 <- network(MigrationFlowsF %>% select(origincode, destcode),
                     matrix.type = "edgelist")

length(miNet1960 %v% "vertex.names")
network.edgecount(miNet1960)
```

*answer:* There are 226 vertices and 20603 edges in this network.

#### (3b)
Is the 1960 migration network more or less connected than in 2000? Explain and use network statistics to support your answer. 

```{r, results = "hide"}
MigrationFlowsF2000 <- MigrationFlows %>%      # create the dataframe for 2000
  filter(sex == "Female", Y2000 > 0)

miNet2000 <- network(MigrationFlowsF2000 %>% select(origincode, destcode),  
                     matrix.type = "edgelist")           # create network object for 2000

net_stats <- do.call(bind_rows, lapply(list(miNet1960, miNet2000), function(net) {
  return(list(
    year = NA,
    density = gden(net),
    diameter = max(geodist(component.largest(net, result = "graph"))$gdist),
    clustering = gtrans(net)
  ))
}))
```

```{r}
net_stats$year <- c(1960, 2000)
knitr::kable(net_stats)
```

*answer:* The 1960 migration network is more connected. The 1960 network had higher density, which means that the proportion of the number of edges to the maximum possible number of edges was higher for the 1960 network. The 1960 network had smaller diameter, which means that it takes a smaller number of steps to connect any two nodes within a subgroup of the 1960 network. Finally, the 1960 network had a higher clustering value, which means that there was a higher tendency for the nodes in the 1960 network to form closed triangles, increasing the inter-relatedness among the nodes.

#### (3c)
Pick your favorite 7-10 countries, then construct a chord diagram to visualize 1960 female migration patterns between these countries. Which country or countries are popular destinations? Which are least popular? Note: To find your country codes, Google search "ISO country codes"" to find a list of three letter codes for all countries. 

*answer:* First, following the day27 activity, I will have to add the data stored in the `Y1960` variable as an edge attribute to the network for 1960 female migration.

```{r}
Y1960_values <- MigrationFlowsF %>%
   arrange(origincode, destcode) %>%
   select(Y1960) %>%
   .$Y1960

set.edge.attribute(miNet1960, "Y1960", Y1960_values)
```

My seven chosen countries are Belarus (BLR), Bulgaria (BGR), Czech Republic (CZE), Hungary (HUN), Moldova (MDA), Poland (POL) and Russia (RUS). I create a subgraph containing only the seven vertices that correspond to these countries. I then use the subgraph to create the chord diagram.

```{r}
myCountries <- c("BLR","BGR","CZE","HUN","MDA","POL","RUS")

indices <- which((miNet1960 %v% "vertex.names") %in% myCountries)
subGraph <- get.inducedSubgraph(miNet1960, indices)

mig_mat <- as.sociomatrix(subGraph, attrname = "Y1960")
mig_mat
set.seed(3)
chordDiagram(mig_mat, symmetric=FALSE)
```

The most popular destination, based on the number of migrations made to the destination, is Belarus. Almost all of the incoming migrants came from Moldova. The least popular destination seems to be the Czech Republic, with no migrants moving into it. 

#### (3d) (Challenge)
Go back to the book data in problem 1. To complete this problem use the joined dataset that contains book ratings (along with info about users and books), but filter out any cases that are missing a `Book.Title`. Construct a network that is defined as follows:

- vertex: a user
- edge: two users are connected if they have at least one book in common (out of all the books they've rated). Use `Book.Title` rather than `ISBN` to determine if two users have rated the same book.
- edge attribute: the number of books that two users have in common
- vertex attribute: the average rating of a user

To construct this network take a random sample of 200 (unique) users from the data frame described at the start of part (3d). Then determine which books (if any) the sampled users have in common to create an adjacency matrix or edgelist that shows the network structure. You should also count how many books each pair has in common and the average rating of each user and add these attributes to your network. (Average user rating is across all books they rated, not just the ones they have in common with other selected users.) Finally, construct a graph (or graphs) that shows both the network structure and these two attributes. Comment on what this graph (or graphs) reveals about the structure of the network and attributes.

Points: Partial credit will be given for your progress made on this problem. E.g. even if you don't get the network relational structure correct you will receive partial points for your attempt (and any work adding vertex and edge attributes).

Comments:

- If creating your `network` object from an edgelist of `User.ID`'s, you should first make the two user ID columns character vectors rather than integer vectors. If you use integers, then `network` will assume that there are unconnected vertices in between all of the integers in your edgelist which will create a *huge* (and incorrect) network!
- If you use an edge list to create the network, you will likely not see 200 vertices in your network unless all 200 randomly selected users are connected to at least one other user. This means you will missing *isolated* vertices. For this problem, that is fine! But if you *really* want to add these unconnected users to the network you can use the function `add.vertices` to add these users to the network. 

*answer:* The joined dataset containing book ratings but without any books with missing titles is called `books_ratings_right2`, created in question 1(c). First, I obtain a vector of unique `User.ID`'s from the dataset. I then select a random sample of 200 users from this vector, and filter the rows from the dataset corresponding to these sampled users.

```{r}
ID_unique <- unique(as.character(books_ratings_right2$User.ID))
set.seed(1)
ID_sample <- sample(ID_unique, size = 200, replace = FALSE)

network_df <- books_ratings_right2 %>%
  filter(as.character(User.ID) %in% ID_sample) %>%
  select(-starts_with("Image"))      # remove unnecessary columns
```

Now I work on creating the edgelist that I will use to construct the network object. First, I create a function that takes in all rows involving the same book title, and returns a string containing the IDs of the users who rated that book. I then group the filtered data frame by `Book.Title` and use the `do()` command to run the function for each book.

```{r}
myFun <- function(group) {
  list_of_users <- str_c(unique(group$User.ID), collapse = ",")
  num_users <- length(unique(group$User.ID))
  return(data_frame(list_of_users, num_users))
}

network_df2 <- network_df %>% group_by(Book.Title) %>% do(myFun(group = .)) %>% 
  arrange(Book.Title)
```

Next, I filter only the rows that have user lists that exceed 1, since only the users in such lists will have edges between them. I then use the command `combn()` to generate all pair-wise combinations of user IDs within each list. I then store these pairwise combinations in a data frame and remove duplicates, which may arise when the same two users rate more than one book. The resulting data frame is the edge list for the network.

```{r}
network_df3 <- network_df2 %>% filter(num_users > 1)

myList <- apply(network_df3, MARGIN = 1, FUN = function(row) {
  ID_vector <- unlist(str_split(row[2], pattern = ","))
})

myList2 <- lapply(myList, FUN = function(userList) {
  combn(userList, m = 2, simplify = FALSE)
})

edgeList_dup  <- as.data.frame(matrix(unlist(myList2), ncol = 2, byrow = TRUE))
edgeList <- edgeList_dup %>%
  unique() %>%
  rename(User1 = V1, User2 = V2)
```

I create the network object using `edgeList` and draw a plot of the network to check if it looks okay.

```{r}
user_network <- network(edgeList, matrix.type = "edgelist", directed = FALSE)
plot(user_network)
```

Now I work on the edge attribute. I first count the number of duplicates that occur within the initial, non-unique edgelist. These values correspond to the number of books that are common to a pair of raters. I then join these values to the unique edgelist, sort the edgelist by the first and second vertex, and then extract the number of common books as a vector. I then set the edge attribute for my network object using this vector.

```{r}
dup_counts <- edgeList_dup %>% 
  mutate(user_list = str_c(V1, V2, sep = ",")) %>%
  count(user_list) %>%
  arrange(desc(n))

num_common_books <- edgeList %>% 
  mutate(user_list = str_c(User1, User2, sep = ",")) %>%
  left_join(dup_counts, by = "user_list") %>%
  arrange(User1, User2) %>%
  .$n

set.edge.attribute(user_network, "Common_books", num_common_books)
```

Now I work on the vertex attribute. I first return to the data frame `network_df` which contains all ratings by the 200 sampled users. I then group by a character version of `User.ID` and compute the average rating for each `User.ID`. Next, I check which of the 200 sampled `User.ID`s actually made it onto the graph, and appended the average ratings for these raters as a vertex attribute.

```{r}
avg_ratings <- network_df %>%
  mutate(User.ID = as.character(User.ID)) %>%
  group_by(User.ID) %>%
  summarise(num_ratings = n(), avg_rating = mean(Book.Rating)) %>%
  arrange(User.ID)
  
indices2 <- which(avg_ratings$User.ID %in% (user_network %v% "vertex.names"))

avg_ratings_values <- avg_ratings %>%
  slice(indices2) %>%
  .$avg_rating

set.vertex.attribute(user_network, "Average_rating", avg_ratings_values)
```

Here, I plot the network object, showing the required vertex and edge attribute. The size of the vertex is proportional to the average rating given by the rater, and the thickness of the edge corresponds to the number of books common to the two raters.

```{r, fig.width = 10, fig.height = 10}
gplot(user_network, usearrows = FALSE, vertex.cex = avg_ratings_values/4,
      edge.lwd = num_common_books)
```

We can make three conclusions from the graph:

- Raters that give very low ratings (very small vertex) tend to be highly connected with other raters. That is, these raters have rated books that other raters have also rated. In constrast, raters that give higher ratings (large vertices) tend to be less connected to other raters, and many are in fact terminal vertices.

- The thickness of the edge does not seem to correlate with the size of the vertices that the edge joins. In other words, there is no correlation between the number of common books between two raters and the average rating they give.

- The distribution of the degrees of the vertices is highly right skewed. In particular, there is one rater who is highly connected with many other raters (degree more than 20), whereas the majority of the raters are connected to only one or two other raters.